<!DOCTYPE html>

<html lang="en">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <head>
        <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" rel="stylesheet">
        <link href="../styles.css" rel="stylesheet">
        <script>
          MathJax = {
            tex: {
              inlineMath: [['$', '$'], ['\\(', '\\)']]
            }
          };
          </script>
        <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
        </script>
        <title>proofs</title>
    </head>

    <div class="container-fluid">
      <div id="side-bar">
        <a href="javascript:void(0)" class="closebtn" onclick="closeNav()">&times;</a>
        <a class="navitem" href="../index.html">home</a> 
        <a class="navitem" href="../static/music2.html">music</a>
        <a class="navitem" href="../static/articles.html">articles</a>
        <a class="navitem" href="../static/poetry.html">poetry</a>
        <a class="navitem" href="../static/about.html">about</a>

      </div>
      <div class="rtcle">
        <article>
            <h1 id="article-title" style = "transition:0.2s; margin-bottom:30px;">A Collection of Proofs</h1>
            <h5 id="article-title" style="transition:0.2s; color:pink;">Chinese Remainder Theorem</h1>
            <p class="theorem-statement">Let $m$, $n$, $a$, $b$ be integers and assume that $\text{gcd}(m, n) = 1$, then one can conclude that 
            there exists an integer $x$ for which $x \equiv a\ (\text{mod}\ m)$ and $x \equiv b\ (\text{mod}\ n)$.</p>
            <p><span style="font-style: italic;">Proof:</span>  Let $m\mathbb{Z}$ and $n\mathbb{Z}$ be the sets of all multiples of $m$ and $n$ respectively.
            and now we define $m\mathbb{Z} + n\mathbb{Z} = \{ms + nr\ |\ s,r\in\mathbb{Z}\}$. Now, since $\text{gcd}(m, n) = 1$,
            every integer can be expressed as a linear combination of $m$ and $n$ since the gcd of two numbers is their 
            smallest positive linear combination. More abstractly, all subgroups $H$ of $\mathbb{Z}$ are of the form 
            $z\mathbb{Z}, z\in\mathbb{Z}$. $m\mathbb{Z} + n\mathbb{Z}$ forms a subgroup and therefore follows such form, 
            for which $z = 1$. So $m\mathbb{Z} + n\mathbb{Z} = \mathbb{Z}$. Now both $a$ and $b$ can be written
            as linear combinations of $m$ and $n$: $a = mr + ns$ and $b = mr' + ns'$. All we have to do now is simply
            choose an $x \in \mathbb{Z}$: $x = mr' + ns$ such that $a - x$ divides $m$ and $b - x$ divides $n$. And the proof
            is now complete.</p>
            <hr>
            <br>
            <h5 id="article-title" style="transition:0.2s; color:pink;">Group of order $p^n$ must contain an element of order $p$.</h1>
            <p class="theorem-statement">Let $G$ be a group with order $p^n$, where $p$ is a prime number, then it must contain an element of order $p$.</p>
            <p><span style="font-style: italic;">Proof:</span> By Lagrange's Theorem, the order of a subgroup $H$ of $G$ divides the order
            of $G$. Let $C_e$ be the cyclic group generated by any arbitrary element of $G$, then Lagrange's Theorem implies that the order of
            $C_e$ is in the set 
            <span class="block-equation">$P = \{1, p, p^2, ... p^{n-1}, p^n \}$</span>
            For any arbitrary element $e\in G$, $C_e$ is going to be of the form 
            <span class="block-equation">$\{1, e, e^2. ... ,e^{p^m-2}, e^{p^m -1} \}$</span>
            for some integer $m\in \{1, 2, ... n\}$. Now we can choose the element $e' = e^{p^{m-1}}$ within $C_e$ 
            and form $C_{e'}$. Now note that $|C_{e'}|$ is always going to be $p$; this is true because
            $|C_{e'}|$ is given by $e'^{|C_{e'}|} = e^{p^{m-1}|C_{e'}|}= 1$, where $1$ is the identity 
            element of $G$. Now we fall back on $C_e$ and observe that $e^{p^m} = 1$ as well. Therefore 
            $p^{m-1}|C_{e'}| = p^m \Rightarrow \boxed{|C_{e'}| = p}$. Since we can form a cyclic group with
            an order in the set $P$ from any element of $G$, and for any cyclic group generated from an element of $G$,
            we can always choose an element $e'\in C_{e'}\subseteq G$ of order $p$, there must exist an element of order $p$ in $G$.
            The proof is now complete.</p>
            <hr>
            <br>
            <h5 id="article-title" style="transition:0.2s; color:pink;">Characterizations of the Determinant</h1>
            <p>
              Let $A$ be an $n \times n$ matrix, and let $\text{det}$ denote the determinant operation, then
              <ol> 
                <li> $\text{det}$ is linear in the rows of $A$ </li>
                <li> if $A$ contains two adjacent rows that are equal, then $\text{det}A = 0$</li>
              </ol>
              "Linearity" in the rows of $A$ means that $\text{det}\begin{pmatrix} \vdots \\ -aX + bY- \\ \vdots \end{pmatrix}
              = a\cdot\text{det}\begin{pmatrix} \vdots \\ -X-\\ \vdots \end{pmatrix} + b\cdot\text{det}\begin{pmatrix} \vdots \\ -Y- \\ \vdots \end{pmatrix}$,
               where $a$ and $b$ are arbitrary real constants, and $X$ and $Y$ are row vectors.
            </p>
            <p><span style="font-style: italic;">Proof:</span> 
              <ol>
                <li> 
                  The proof will be by induction. Let $P(n)$ be the predicate that a square matrix of dimensions
                  $n \times n$ satisfy characterization 1. The base case $n = 1$ is trivially satisfied, for 
                  <span class="block-equation">$\text{det} \begin{bmatrix} a + b \end{bmatrix} = a + b = \text{det}\begin{bmatrix}a\end{bmatrix} + \text{det}\begin{bmatrix} b \end{bmatrix}$ </span>
                  is true for a $1 \times 1$ matrix. Now assume that $P(k)$ is true for some arbitrary $k \in \mathbb{N}$.
                  We now turn our attention to a $(k + 1) \times (k + 1)$ matrix that has the form $\begin{pmatrix} \vdots \\ -aX + bY- \\ \vdots \end{pmatrix}$, where $X + Y$ 
                  is in the $z$-th row. Using 
                  cofactor expansion down the leftmost column, we have that 
                  <span class="block-equation">$\text{det}A = a_{11}\det{A_{11}} - a_{21}\det{A_{21}} + \dots \pm (ax_1 + by_1)\det{A_{z1}} \mp \dots$</span>
                  Let $A_{\bar{X}}$ denote $A$ without the $X$ component in the $z$-th row, and similar logic applies to $A_{\bar{Y}}$.
                  By our inductive hypothesis, $\det{A_{j1}}$ can be written as $b\det{A_{j1\bar{X}}} + a\det{A_{j1\bar{Y}}}$ for all $j \neq z$. For $j = z$, we
                  can expand the term $(ax_1 + by_1)\det{A_{z1}} = ax_1\det{A_{z1}} + by_1\det{A_{z1}}$. Now we can rewrite $\det{A}$ 
                  <span class="block-equation"> $ a_{11}(b\det{A_{11\bar{X}}} + a\det{A_{11\bar{Y}}}) - \dots \pm (x_1 \det A_{z1} + y_1 \det A_{z1}) \mp \dots $ </span>
                  we can now regroup the terms in the sum as follows:
                  <span class="block-equation"> $\det A = (a_{11}b\det{A_{11\bar{X}}} - \dots \pm by_1 \det A_{z1} \mp \dots) + (a_{11} a\det A_{11\bar{Y}}-\dots\pm ax_1 \det A_{z1} \mp \dots)$ </span>
                  which is just the same as 
                  <span class="block-equation"> $\det A = b \det A_{\bar{X}} + a\det A_{\bar{Y}}$</span>
                  since arbitrary removal of components from the $z-th$ row does not affect the value of $\det A_{z1}$.
                  So $P(k + 1)$ holds and the proof by induction is complete.

                </li>
                <li>
                  The proof will be by induction. Let $P(n)$ be the predicate that an $n \times n$ matrix satisfies
                  characterization 2. The base case $n = 2$ is satisfied from $\det \begin{pmatrix} a & b \\ a & b \end{pmatrix} = ab - ba = 0$. Now assume
                  that the predicate is true for some arbitrary $k \in \mathbb{N}$. We now turn our attention to a $(k + 1) \times (k + 1)$ matrix
                  of the form $\begin{pmatrix} \vdots \\ -X- \\ -X- \\ \vdots \end{pmatrix}$ with $X$ in the $p$-th and $(p + 1)$-th rows.
                   Expanding down the leftmost column gives
                  <span class="block-equation">$\det A = a_{11}\det A_{11} - \dots \pm x_1 \det A_{p1} \mp x_1 \det A_{(p+1)1} \pm \dots $</span>
                  In general, all but the two terms involving $p$ and $p + 1$ vanishes due to our inductive assumption. So that
                  <span class="block-equation"> $\det A = \pm x_1 \det A_{p1} \mp x_1 \det A_{(p+1)1}$ </span>
                  But $A_{p1}$ and $A_{(p+1)1}$ are the same matrix. So they cancel out. $\det A = 0$. $P(k + 1)$ holds. 
                  Proof by induction is complete.
                </li>
              </ol>
            </p>
            <hr>
            <br>

            <h5 style="transition:0.2s; color:pink;">Determinant of a Transposition is the Determinant of the Original (Square) Matrix</h1>
            <p class="theorem-statement">Let $A$ be an $n \times n$ matrix, then $\det A = \det A^T$.</p>
            <p><span style="font-style: italic;">Proof:</span>  
            We will first show that for any elementary matrix $E$, $\det E = \det E^T$. Then we show that for any square matrix $A'$ in
            its reduced echelon-row form (RREF), $\det A' = \det {A'}^T$. This will allow us to complete the proof.
            <br><br>
            There are three types of elementary matrices. The first of which is one that, when right multiplied by a matrix,
            adds a row to another to replace the latter row. This type of matrix is identical to the identity matrix with an arbitrary 
            entry of $1$ at some location not on the diagonal. For such a matrix $E_1$, it can easily be calculated that $\det E_1 = 1$.
            Now note that the transpose of an $E_1$ matrix is still a matrix of the same form. Therefore $\det E_1 = \det E^T_1 = 1$. The same
            logic can be applied to the other two types of elementary matrices, namely the swapping-rows matrices and the scale-rows-by-constant matrices.
            <br><br>
            Now, consider an RREF square matrix $R$. It either has a pivot in every column or it does not. If it does
            then it is an identity matrix. If it does not, there is at least one row, namely the bottom, with all zero entries. In the first case,
            the transpose of an identity matrix is itself, so $\det R = \det R^T$ in that case. In the second case, any matrix with a 
            zero row has a determinant of zero by the linearity of determinants. So that $\det R = 0$. Now we can left multiply $R$ by a swapping-rows 
            matrix $E_2$ such that the top row ends up being all zeros, so that $\det (E_2R)^T = 0$ from the leftmost column of all zero entries. 
            $\det [(E_2R)^T] = \det (R^T E^T_2) = \det R^T \det E^T_2 = -1 \cdot \det R^T$ = 0. Therefore we see that $\det R^T = \det R = 0$.
            So, in either case, $\det R = \det R^T$.
            <br><br>
            Any square matrix can be reduced down to its RREF form via multiplying it by a series of elementary matrices.
            This process is reversible since all elementary matrices are recersible. Therefore, if we have some square matrix 
            $A$, there exists a sequece of elementary matrices $E_1\dots E_k$ such that the following relation involving $A$'s RREF
            matrix $R$ holds:
            <span class="block-equation">$A = E_1 \dots E_k R$.</span>
            The transpose of $A$ is thus  
            <span class="block-equation">$A^T = R^T E^T_k \cdots E^T_1$.</span>
            $\det A = \det E_1 \dots \det E_k \det R$ and $\det A^T = \det E^T_1 \dots \det E^T_k \det R^T$.
            By the discussion above, these two products are equivalent. Therefore, $\det A = \det A^T$. The proof is complete. 
            </p>
            <hr>
            <br>
            <h5 id="article-title" style="transition:0.2s; color:pink;">Fermat's Theorem</h1>
            <p class="theorem-statement">Let $p$ be an arbitrary prime number, then for any integer $a$, $a^p \equiv a \ (\text{mod}\  p)$.</p>
            <p><span style="font-style: italic;">Proof:</span> 
            Let $\mathbb{F}_p$ denote the field consisting of the congruence classes modulo $p$. So 
            the order of $\mathbb{F}_p$ is $p$. As a property of fields, the set $\mathbb{F}_p - \{0\}$ forms 
            a group under multiplication. Let this group be denoted by $\mathbb{F}^{\times}_p$. Let the congruence 
            class of an integer $a$ be denoted as $\bar{a}$.
            <br><br> 
            We will first show that, for any non-zero congruence class $\bar{a}$, $\bar{a}^{p-1} = \bar{1}$.
            Equivalently, $a^{p-1} = 1\ (\text{mod}\ p)$. Then it remains a trivial matter to show the final result.
            <br><br>
            Since $\mathbb{F}^{\times}_p$ is a group under multiplication, every element in $\mathbb{F}^{\times}_p$
            has a multiplicative inverse. Also, the order of $\mathbb{F}^{\times}_p$ is $p - 1$. By Lagrange's Theorem, 
            the order of any cyclic subgroup generated by any of the elements divides $p - 1$. Let $C_{\bar{a}}$ denote the cyclic
            subgroup generated by the congruence class $\bar{a}$, then we can conclude that $|C_{\bar{a}}| = \frac{p - 1}{z}$ for some 
            $z \in \mathbb{Z}$. For instance, if $z = 1$, then it is trivial that $\bar{a}^{p-1} = \bar{1}$ by the property of 
            cyclic groups. In the most general case, we are guaranteed to have that 
            <span class="block-equation"> $\bar{a}^{p-1} =\bar{a}^{z|C_{\bar{a}}|} =  \left(\bar{a}^{|C_{\bar{a}}|}\right)^z = \bar{1}^z = \bar{1}$.</span>
            It can be shown that if $a \equiv a'\ (\text{mod}\ p)$ and $b \equiv b'\ (\text{mod}\ p)$, then $ab = a'b' \ (\text{mod}\ p)$. 
            Therefore we can simply multiply both sides of the equation above by $\bar{a}$ to get that 
            <span class="block-equation"> $\bar{a}^p = \bar{1}$.</span>
            and equivalently 
            <span class="block-equation"> $a^{p-1} \equiv a\ (\text{mod}\ p)$</span>
            The proof is now complete. 
            </p>
            <hr>
            <br>
            <h5 id="article-title" style="transition:0.2s; color:pink;">Wilson's Theorem</h1>
            <p class="theorem-statement">Let $p$ be an arbitrary prime number, then $(p-1)!\equiv -1\ (\text{mod}\ p)$.</p>
            <p><span style="font-style: italic;">Proof:</span> The notations in this proof will stay consistent as that used 
            in the proof of Fermat's Theorem above. 
            <br><br> 
            We will show that the product of all non-zero congruence classes is equal to $\overline{-1}$.
            As $\mathbb{F}^{\times}_p$ is a group under multiplication, every element in it has a multiplicative 
            inverse. 
            <br><br>
            Suppose that $a$ and $a'$ have the same inverse $b$ such that $ab \equiv 1$ and $a'b \equiv 1$. 
            This is the same as saying that $a \equiv b^{-1}$ and $a' \equiv b^{-1}$. This in turn, is equivalent to
            saying that $b^{-1} = a + np$ and $b^{-1} = a' + sp$ for some $n, s\in\mathbb{Z}$. Subtracting these two equations we find that 
            <span class="block-equation">$0 = (a-a') + (n - s)p$.</span>
            Rearranging gives 
            <span class="block-equation">$a-a' = (n-s)p$.</span>
            This implies that $a \equiv a'$. So we have found that if two numbers have the same multiplicative 
            inverse, then they belong to the same congruence class. This implies that it is possible to find 
            distinct inverses for each element in $\mathbb{F}^{\times}_p$.
            <br><br> 
            So now we can manipulate the product $\overline{(p-1)!}$. Note that since the congruence classes partition the 
            subset of integers that are not multiplies of $p$, $-1$ is guaranteed to end up in one of the elements of $\mathbb{F}^{\times}_p$.
            In other words, $\exists\ b \in \mathbb{F}^{\times}_p\ \bar{b} = \overline{-1}.$ We can manipulate the product as follows:
            For any non-zero congruence class that is not $\bar{1}$ or $\bar{-1}$, pair them up with their unique multiplicative inverses by 
            the associativity and commutativity inherited from the property of fields, such that the product will look like 
            <span class="block-equation"> $\overline{(p-1)!} = \bar{1} \cdot \prod_{\bar{i} \in \mathbb{F}^{\times}_p, \bar{i}\neq \overline{-1}, \overline{1}} (\bar{i} \cdot \bar{i}^{-1})\cdot \overline{-1} = \overline{-1}$.</span>

            The proof is now complete. 
            </p>
            <hr>
            <div class="references">
                <h4>References</h4>
                <ol>
                    <li>Artin, Michael. <span style="font-style:italic">Algebra.</span></cite></li>
                    <li>placeholder reference <cite><a href="https://www.w3schools.com/tags/tag_hr.asp">w3schools hr tag tutorial</a></cite></li>
                </ol>
            </div>
         </article>
        </div>
    
      <span class="sidebar-open" onclick="openNav()">&#9776; nav</span>
    </div>

    <!-- remember to make the texts responsive to screen sizes!!!! -->
    
    <script>
      function openNav() {
        document.getElementById("side-bar").style.width = "200px";
      }
      
      function closeNav() {
        document.getElementById("side-bar").style.width = "0";
      }

      
    </script>



</html>
